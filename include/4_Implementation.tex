\chapter{Implementation}\label{ch:implementation}
In this chapter, we describe the process of applying the design as described in \Cref{ch:design} to our implementation
of the Certificate Management System.

\section{Existing Prototype}\label{sec:exisingPrototype}
Dr.\ Wachs et al.\ already implemented a prototype, that can be used to issue and manage certificates.
This system was used as basis of our work, however several key features were still lacking:

After analyzing the prototype, it seemed overly complex and overly modularized.
Several abstractions do not make sense, i.e.\ it was designed to support different so-called \say{Runtime Abstraction
Layers} (RALs), while none of that abstraction was actually needed.
Furthermore, the project was on only modularized on first glance, but contained several \say{common} modules, which
turned out to be tightly coupled into the whole system.
This made things worse than no modularization at all, since related classes were distributed between modules and
functionality was duplicated between modules.

Also the deployment of the software was rather fragile and consisting of a combination of Bash and Maven files and
separate database configuration instructions, which needed to be executed in the right order.
This resulted in a scary deployment process, which resulted in a virtual machine, which was updated every once in a
while.
This is obviously a bad idea, since any \say{runtime abstraction} prior to that machine was rendered useless.

Furthermore, when looking at the Java code, no clear line of input validation or error propagation boundaries were
visible.
Almost every class had their own \lstinline{null} pointer checks, returning \lstinline{null} themselves in error cases
while only logging the error message.
This resulted in the vast majority of unit tests being dedicated to nullability checks.

Lastly, almost all user management features were not included in the overall design of the system, which already
struggled under its own complexity, while lacking some of the most basic features.
For our work, we decided to first improve the complexity management of the project and then implement the missing
features as described in \Cref{ch:design}.

\section{Complexity Reduction}\label{sec:complexityReduction}
Overall complexity reduction is important to allow efficient long-term support of the project.
To achieve this, we decided to take the following steps to keep the project manageable:
\begin{itemize}
    \item First, reify the overly abstract implementation of many features
    \item Second, bundle competences to reduce cross module functionalities
    \item Third, eliminate overly defensive programming in favour of more compile time checks
\end{itemize}

\subsection*{Reduced Abstraction}
In the beginning of the working group, \citet{maier2015multidevice} explored different application approaches, i.e.\
native non-HTML based clients.
Therefore, different Runtime Abstraction Layers (RALs) were introduced as an abstraction over the different target
platforms.
Similar exploration happened for the backend, which could run on multiple target frameworks, over wich RALs abstract.

This lead to lots of duplicated code, excessive tests, and multiple distinct \say{common} code sections.
For our project, we decided to ditch the RALs and only use a single Spring Boot application.
Java alone provides enough abstraction for future extensibility and one web framework is already enough to support our
application.
Another advantage of this decision is, that we could do user authentication once properly, instead of having to support
a weak abstraction over JavaEE, Spring, CLI, and LDAP authentication.

\subsection*{Bundled Competences}
We also decided to bundle competences in modules, since cross module spanning features are rather hard to maintain,
since changes to those features would also need to happen across boundaries.
These boundaries between subsystems also became much more visible due to the reduced abstraction, since with fewer
classes, cross-module dependencies were much more visible.
For our use-case, we settled with only three subsystems, where two systems are already very visible in the design
(c.f.\ \Cref{fig:systemArchitecture}): The web based frontend and the server-side backend.

We additionally divided the backend into two subsystems.
A certificate management component, which handles the certification conversion, cryptographic signature validation, and
persistent database storage of valid information.
This component generally assumes trusted inputs, which do not need additional external validation.
In contrast, the web component, handles user management, validates inputs, manages access privileges, and provides the
REST interface for the communication with the frontend.

The user-facing frontend module is completely separated and only coupled via the REST API\@.
It is designed to automate the user's certificate process and is described in detail in a separate work by Kordian
Bruck.


\subsection*{Design by Contract}
To reduce complexity in the project even more, we applied the design by contract~\cite{meyer1992applying} methodology,
to eliminate code overhead for the overall \lstinline{null} handling.
At first, we noticed, that around 50\% of all tests were focused on simply handling nullable types, ensuring
\lstinline{null} inputs produce \lstinline{null} outputs.
This inflated overall code size, but provided no actual value.
Additionally, fatal errors or failed assumptions would also commonly result in null values, which then would propagate
throughout the entire system instead of failing locally.

Instead of handling \lstinline{null} values where they indicate exceptional states, we decided to eliminate most usages
of \lstinline{null} and explicitly used Java annotations (\lstinline{@Nullable} or \lstinline{@NotNull}), which support
nullability contracts where such values actually make sense (e.g.\ user not found, optional configuration parameter not
set).
This way, the overall number of paths through the code was significantly reduced and all input verification can be
handled locally, at the time of input.
This additionally provides much better error messages in exceptional cases, where exceptions now do not propagate to
unrelated functions, but are logged right at the point where they are caused.

Many errors, were caused by violated assertions, i.e.\ programming errors or violated preconditions.
Those now cause exceptions, which have the advantage to fail hard, instead of generating propagating \lstinline{null}
values and actually be logged,
instead of being swallowed by generic \lstinline{null} handling code.

\section{Deployment with Gradle}\label{sec:deploymentWithGradle}
The previous build automation system was based on Apache Maven, combined with Makefiles and Bash scripts.
This made the development environment non-portable, but only buildable on Linux and resulted in slow build times and
inflexible extensibility.
Because of this system, multiple separate configuration scripts needed to be executed in the right order, despite having
a build system designed to avoid such troubles.
An integration of modern web development frameworks was also not easily possible in this setup, but would result in
additional manual configuration effort.

In our approach, we handle this by switching to a more capable build system, that can actually manage those build tasks.
A suited and modern alternative build system for Java projects is Gradle, which we used to eliminate the need for
separate scripts, since Gradle can be used to perform arbitrary tasks using the Apache Groovy language.

\subsection*{Automatic Dependency Updates}
Another potentially dangerous consequence of the previous, convoluted build system was, that updating dependencies was
troublesome.
Updating dependencies should actually be easy, especially for a security-critical application like ours.
Gradle has the capabilities of detecting outdated and potentially vulnerable dependencies via the
\say{gradle-versions-plugin}.
This plugin can be used to update the more than 25 different third-party libraries, which were used for this project.

The check for outdated dependencies can then be fully automatically be integrated in regular build jobs, which displays
the newest available versions for open-source repositories.
An example output is shown below.

\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={Example output of the gradle-versions-plugin}]
$ gradle dependencyUpdates
------------------------------------------------------------
: Project Dependency Updates (report to plain text file)
------------------------------------------------------------

The following dependencies are using the latest integration version:
- org.apache.directory.server:apacheds-all:2.0.0-M24
- dfncert:client:1.9.1
- io.jsonwebtoken:jjwt:0.9.0
[...]

The following dependencies have later milestone versions:
- org.bouncycastle:bcmail-jdk15on [1.56 -> 1.59]
- org.bouncycastle:bcpg-jdk15on [1.56 -> 1.59]
- org.mariadb.jdbc:mariadb-java-client [2.2.3 -> 2.2.5]
[...]
\end{lstlisting}
\end{minipage}

While handling the dependency updates, we noticed problems with updating individual dependencies, since the dependencies
have internal dependencies themselves, commonly described as \say{Dependency Hell}~\cite{jang2006linux}.

In out application, dependency updates were especially time consuming and frustrating, since mismatched versions of the
Spring Framework and dependent dependencies can lead to \lstinline{ClassNotFoundException}s at runtime.
This is inherently due to Java's implementation of lazily loading classes~\cite{gosling2014java}.

Our implementation solves this by specifying version numbers globally, which removes the possibility of \say{missing}
to update one dependent version.
This couples compatible versions of the whole Spring Framework and Log4J together which automatically keeps the versions
compatible.

\section{Database Versioning and Migrations}\label{sec:databaseVersioningAndMigrations}
While working on the database, we quickly noticed, that the database access needs severe improvement.
Since every change in the database schema resulted in multiple changes and breakages, all of which only were detected at
runtime.
Since none of the existing tests actually verified the database, this only manifested in hard to debug crashes.
Also, no good upgrade path is available, since there war no versioning information available in the database.
The jackhammer method of exporting the whole database and recreating it each time the schemas were updated was quickly
discarded.

Instead, we decided to use two industry standard dependencies to manage those problems: jOOQ~\cite{jooq} and
flyway~\cite{flyway}

\subsection*{Object Relational Mapper}
The idea behind jOOQ is static class generation to check matching models at compile time and features an alternative
Domain Specific Language (DSL) to SQL\@.
This idea provides several advantages over classic Java Database Connectivity (JDBC) in combination with raw SQL:
\begin{itemize}
\item Type safety between the database model and the Java application
\item Support for different databases
\item Enterprise support, if needed for enterprise databases
\end{itemize}

JDBC has the problem, that it provides an abstraction to connect to different databases, but does not abstract the
SQL queries, where some databases might support different subsets of operations.
Additionally, it only provides connectivity, but the data parsing into objects or datastructures still needs to be done
manually.
JOOQ here has the advantage to shift parsing errors from the testing into the compilation process, which allows to
detect bugs and schema errors as early as possible.

\subsection*{Database Migrations}
To improve the previously non-existent migration paths of schemas, we introduced Flyway to version the database schema
and explicitly state, in which state the database should be.
Flyway provides automate creation of database schema and migration of old data via a Gradle task, that can be defined to
automatically apply the correct migrations for the currently deployed version.
This severely reduces the work, that needs to be done when updating the database schema.

All in all, this makes the future development of the application much easier, since for each change to the database,
only a single migration needs to be written.
Subsequently, the matching database models are generated and any mismatches are immediately reported as compiler errors.

\section{Core Features}\label{sec:coreFeatures}

The work in the last sections described log term integration of the development and maintenance process into existing
systems.
In this section we describe the added additional features, which are needed to seamlessly integrate into existing
organization's workflows.

\subsection*{Authentication}
To authenticate users, we implemented LDAP authentication according to RFC4513~\cite{RFC4513}.
Additionally, we use this authenticated connection to retrieve the necessary user information to generate and validate
certificates from the directory service.
For user information, we support two implementations, either standard conforming LDAP users or user information as
implemented by Microsoft's Active Directory.

For the auth process, we used the ApacheDS library, which provides standard connectivity to LDAP services.
Based on the connectivity, we can authenticate users with the service using standard username and password over TLS,
e.g.\ to TUM's Active Directory.

We then can access the users information fields, as specified by RFC4519~\cite{RFC4519}.
However, many organizations use non standard conforming implementations, most prominently Microsoft's Active Directory,
which handles email addresses quite differently.
Luckily, Microsoft publicly documents its implementation\footnote{https://support.microsoft.com/en-us/help/3190357/}.
The RFC defines the \lstinline{mail} field to contain email addresses, but in Active Directories only the primary
address is listed here.
However, there is an additional, non-standard, multi-value \lstinline{proxyAddresses} attribute, which also contains the
primary email address, but prefixed with uppercase \lstinline{SMTP:}.
Secondary email addresses are contained also contained in this attribute, but prefixed with lowercase \lstinline{smtp:}.

To explain this behaviour, Microsoft historically supported now obsolete mail addresses like X.400, X.500, cc:Mail,
MS-Mail, GroupWise, and others.
To solve the ambiguity of plain addresses, the addresses are stored as URIs~\cite{RFC3986}, with a protocol prefixed.

Since non of those alternative protocols are actively in use or supported, we chose to ignore those and assume a
SMTP-only configuration.
For our use-case all SMTP addresses are relevant certificate subjects, so we try to extract and handle all addresses.

\subsection*{Certificate Publishing}
To publish certificates containing the authenticated public key information of users, we use the same directory
services, which already contain existing information about the user.
In our system, we generate matching certificates, which cryptographically ensures authenticity of this information.
By publishing though the same source of information, we do not actually add sensitive personal information, but only
enhance it by guaranteeing its authenticity.

The actual publishment of certificates happens via the same LDAP protocol, i.e.\ with a RFC4523~\cite{RFC4523} compliant
user certificate in the \lstinline{userCertificate} field.
This field contains the users certificate in DER encoded binary format.
It also supports multi-value attributes, such that users with multiple certificates, e.g.\ when generating several new
certificates, are handled correctly by the directory service.

RFC4523 compliant clients with access to the directory can now automatically fetch this \lstinline{userCertificate} and
start to encrypt and validate emails to and from that user.
E.g.\ at TUM, when adding the internal active directory address book to Mozilla Thunderbird, the client is able to
automatically fetch the certificate, validate signed emails from TUM users, and send encrypted emails to them.
The only aspect, which is still not automated is the one-time correct email client configuration.

\subsection*{Automation}
In this particular work, we actually only provide APIs, which then can be used to maximally automate the user-facing
process in the frontend.
Interested readers should consult the corresponding work about the frontend.

For the backend, we automated the process as much as possible.
I.e.\ we assume automatic approval of certificates, since all user information was correctly verified with the identity
management.
Manual approval of certificates is slow and drastically breaks the flow and user's incentive to actually use
certificates.
LDAP user authentication provides strong internal guarantees about the user's identity and also technically creates a
chain of trust between manual creation of that user when entering the identity management and generating user
certificates.

Automatic publishing of generated certificates furthermore allows privacy-by-default behaviour for most email clients.
Certificate publishing is also not a privacy concern, since user information is already stored in the directory service,
just not yet cryptographically signed.
