\chapter{Implementation}\label{ch:implementation}
In this chapter, we describe the process of applying the design as described in \Cref{ch:design} to the
Certificate Management System.

\section{Exising Prototype}\label{sec:exisingPrototype}
Dr.\ Wachs et al.\ already implemented a prototype, that can be used to issue and manage certificates.
This system can be used as basis of our work, however several key features are still lacking:

After analyzing the prototype, it seemed overly complex and overly modularized.
Several abstractions do not make sense, i.e.\ it was designed to support different so-called \say{Runtime Abstraction
Layers} (RALs), while none of that abstraction was needed.
Furthermore, the project was on first sight modularized with several \say{common} modules, which turned out to be
tightly coupled into the whole system.
This made things worse than no modularization at all, since related classes were distributed between modules and
functionality was duplicated between modules.

Also the deployment of the software was rather fragile and consisting of a combination of bash and maven files and
separate database configuration instructions, which needed to be executed in the right order.
This resulted in a scary deployment process, which resulted in a virtual machine, which was updated every once in a
while.
This is obviously a bad idea, since any \say{runtime abstraction} prior to that machine was rendered useless.

Furthermore, when looking at the Java code, no clear lines of input validation or error propagation were visible.
Almost every class had their own null pointer checks, and returning null themselves in error cases and only logging the
error message.
This resulted in the vast majority of unit tests being dedicated to nullability checks.

Lastly, almost all user management features were not included in the overall design of the system, which already
struggled under its own complexity, while lacking some of the most basic features.
For our work, we decided to improove the complexity management of the project and implement the missing features as
described in \Cref{ch:design}.

\section{Complexity Reduction}\label{sec:complexityReduction}
As a first step, we decided to reduce the overall complexity to allow efficient long-term support of the project.
% first: reify overly abstract implementations
% second: bundle competences to reduce functionality spanning multiple modules
% third: eliminate overly defensive programming
% TODO
\subsection*{Reduced Abstraction}
No RAL, only a single Spring Boot Application
% different application approaches were explored by maier2015multidevice. RALs try to abstract over different
% implementations of the backend.
% Lead to lots of duplicated code, and excessive tests, and multiple distinct "common" code sections.
% Java already provides enough abstraction for future extensibility and we settled with one design.
% User authentication done once properly, instead of supporting JavaEE, Spring, and LDAP authantication

\subsection*{Bundled Competences}
% module spanning features are hard to maintain
% reduced abstraction also resulted in clearer boundaries between subsystems
% 3 subsystems: certificate management component, which hanldes the certification conversion and cryptographic signage
% and validation. Assumes trusted inputs, which do not need external validation.
% Web frontend, which handles user management, validates inputs, and access privileges. Provides its services as REST API
% Completely separate user-facing frontend module, only coupled via the REST API. Designed to automate certificate process.
Bundled competences with reduced modularization: Certificate Management System and REST-API and completely separate
Frontend module

\subsection*{Design by Contract}
% TODO: proper sentences
Applied the design by contract~\cite{meyer1992applying} methodology, to reduce the overall \lstinline{null} handling.
Around 50\% of all tests were focused on handling nullable types, ensuring null input produces null output and
generally, that errors result in null values.
However, since for almost all cases, null values indicate input errors and should not propagate throughout the entire
system.
We therefore eliminated most usages of null and used Java Annotations, supporting nullability contracts
(\lstinline{@Nullable} or \lstinline{@NotNull}).
This way, the overall number of paths through the code is reduced significantly reduced and all input verification is
actually handled at the time of input, which additionally provides much better error messages.

Additionally, many errors caused by violated assertions, i.e.\ programming errors, now cause exceptions.
Exceptions have the advantage to fail hard, instead of generating propagating null values and actually be logged,
instead of being swallowed by generic null handling code.

\section{Deployment With Gradle}\label{sec:deploymentWithGradle}
The previous build automation system was based on Apache Maven, combined with Makefiles and Bash scripts.
This made the development environment non-portable, but only buildable on Linux and resulted in slow build times and
inflexible extensibility.
Because of this system, multiple separate configuration scripts needed to be executed in the right order, despite having
a build system designed to avoid such troubles.
An integration of modern web development frameworks was also not easily possible in this setup, but would result in
additional manual configuration effort.

In our approach, we handle this by switching to a more capable build system, that can actually manage those build tasks.
A suited and modern alternative build system for Java Projects is Gradle, which we used to eliminate the need for
separate scripts, since Gradle can be used to perform arbitrary tasks using the Apache Groovy language.

\subsection*{Automatic Dependency Updates}
Another potentially dangerous consequence of the previous, convoluted build system was, that updating dependencies was
troublesome.
Updating dependencies should actually be easy, especially for a security-critical application like ours.
Gradle has the capabilities of detecting outdated and potentially vulnerable dependencies via the
\say{gradle-versions-plugin}.
This plugin can be used to update the more than 25 different third-party libraries, which were used for this project.

The check for outdated dependencies can then be fully automatically be integrated in regular build jobs, which then
display the newest available versions for open-source repositories.
An example output is shown below.

\begin{lstlisting}
$ gradle dependencyUpdates
------------------------------------------------------------
: Project Dependency Updates (report to plain text file)
------------------------------------------------------------

The following dependencies are using the latest integration version:
- org.apache.directory.server:apacheds-all:2.0.0-M24
- dfncert:client:1.9.1
- io.jsonwebtoken:jjwt:0.9.0
[...]

The following dependencies have later milestone versions:
- org.bouncycastle:bcmail-jdk15on [1.56 -> 1.59]
- org.bouncycastle:bcpg-jdk15on [1.56 -> 1.59]
- org.mariadb.jdbc:mariadb-java-client [2.2.3 -> 2.2.5]
[...]
\end{lstlisting}

While handling the dependency updates, we noticed problems with updating individual dependencies, since the dependencies
have internal dependencies themselves, commonly described as \say{Dependency Hell}~\cite{jang2006linux}.

Especially time consuming and frustrating, since mismatched versions of the Spring Framework and dependent dependencies
can lead to a \lstinline{ClassNotFoundException} at runtime.
This is inherently due to Java's implementation of lazily loading classes~\cite{gosling2014java}.

Our implementation solves this, by specifying version numbers globally, which removes the possibility of \say{missing}
to update one dependent version.
This couples compatible versions of the Spring Framework and Log4J together which should automatically keep the versions
compatible.

\section{Database Versioning And Migrations}\label{sec:databaseVersioningAndMigrations}
While working on the database, we quickly noticed, that the database access needs severe improvement.
Since every change in the database schema resulted in multiple changes in the breakages, all of which only happened at
runtime.
Since none of the existing tests actually verified the database, this only manifested at runtime in hard to debug
crashes.
Also, no good upgrade path is available, since there is no no versioning information available in the database.
The jackhammer method of exporting the whole database and recreating it each time the schema is updated was quickly
discarded.

Instead, we decided to use two industry standard dependencies: jOOQ~\cite{jooq} and flyway~\cite{flyway}

\subsection*{Object Relational Mapper}
The idea behind jOOQ is static class generation to check matching models at compile time and features an alternative
Domain Specific Language (DSL) to SQL\@.
This idea provides several advantages over classic Java Database Connectivity (JDBC) in combination with raw SQL:
\begin{itemize}
\item Type safety between the database model and the Java application
\item Support for different databases
\item Enterprise support if needed for enterprise databases
\end{itemize}

JDBC has the problem, that it provides an abstraction to connect to different databases, but does not abstract the
SQL queries, where some databases might support different subsets of operations.
Additionally, it only provides connectivity, but the data parsing into objects or datastructures still needs to be done
manually.
JOOQ here has the advantage to shift parsing errors from the testing into the compilation process, which allows to
detect bugs and schema errors as early as possible.

\subsection*{Database Migrations}
To improve the previously non-existant migration paths of schemas, we introduced Flyway to version the database schema
and explicitly state, in which state the datbase should be.
Flyway provides automate creation of database schema and migration of old data via a gradle task, that can be defined to
automatically apply the correct migrations for the currently deployed version.
This severely reduces the work, that needs to be done when updating the database schema.

All in all, this makes the future development of the application much easier, since for each change to the database,
only a single migration needs to be written.
Subsequently, the matching database models are generated and any mismatches are immediately reported as compiler errors.

\section{Core Features}\label{sec:coreFeatures}

The work in the last sections described log term integration of the development and maintenance process into existing
systems.
In this section we describe the added additional features, which are needed to seamlessly integrate into existing
organization's workflows.

\subsection*{Authentication}
To authenticate users, we implemented LDAP authentication according to RFC4513~\cite{RFC4513}.
Additionally, we use this authenticated connection to retrieve the necessary user information to generate and validate
certificates from the directory service.
For user information, we support two implementations, either standard conforming LDAP users or as implemented by
Microsoft's Active Directory.

For the auth process, we used the ApacheDS library, which provides standard connectivity to LDAP services.
Based on the connectivity, we can authenticate users with the service using standard username and password over TLS,
e.g.\ to TUM's Active Directory.

We then can access the users information fields, as specified by RFC4519~\cite{RFC4519}.
However, many organizations use non standard conforming implementations, most prominently Microsoft's Active Directory,
which handles email addresses quite differently.
Luckily, Microsoft publicly documents its implementation\footnote{https://support.microsoft.com/en-us/help/3190357/}.
The RFC defines the \lstinline{mail} field to contain email addresses, but in Active Directories only the primary
address is listed here.
However, there is an additional, non-standard multi-value \lstinline{proxyAddresses} attribute, which also contains the
primary email address, but prefixed with uppercase \lstinline{SMTP:}.
Secondary email addresses are contained also contained in this attribute, but prefixed with lowercase \lstinline{smtp:}.

To explain this behaviour, Microsoft historically supported now obsolete mail addresses like X.400, X.500, cc:Mail,
MS-Mail, GroupWise, and others.
To solve the ambiguity of plain addresses, the addresses are stored as URIs~\cite{RFC3986}, prefixed with the protocol.

Since non of those alternative protocols are actively in use or supported, thus we chose to ignore those and assume a
SMTP only configuration.
For our use-case all SMTP addresses are relevant certificate subjects, so we try to extract and handle all addresses.

\subsection*{Certificate Publishing}
% TODO
User information already present in directory services.
Matching certificate information, which cryptographically ensures authenticity of this information added with the
certificate publishing component.
Publishing of certificates via the same LDAP service, i.e.\ publishing of RFC4523~\cite{RFC4523} compliant user
certificate.
\lstinline{userCertificate} field, containing the certificate in DER encoded binary format.

This field can also be multi-value, such that users with multiple certificates, e.g.\ when generating new certificates
are handled correctly.
RFC4523 compliant clients can now automatically fetch this \lstinline{userCertificate} and start to encrypt emails to
that clients.
E.g. at TUM, when adding the internal address book to Mozilla Thunderbird, the client is able to fetch the certificate,
validate signed emails from TUM users and send encrypted emails to them.
Only requires correct user-side client configuration.

\subsection*{Automation}
% TODO: in this work, we actually only provide APIs, which are maximally automated
% more user facing automation was done on the frontend, so refer to that.
Automatic approval of certificates assumed as default.
Might have political reasons, but LDAP user authentication provides strong internal guarantees about the user's
identity.

Automatic publishing of generated certificates allows privacy-by-default behaviour for most clients.
Certificate publishing is also not a privacy concern, since user information is already stored in the directory service,
but not yet cryptographically signed.
